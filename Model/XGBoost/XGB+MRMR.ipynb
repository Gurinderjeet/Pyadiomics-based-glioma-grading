{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GssaepCpQfOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install xgboost\n",
        "!pip install numpy Cython\n",
        "!pip install pymrmr\n",
        "\n",
        "import io\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import statistics\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "import pymrmr\n",
        "\n",
        "\n",
        "hgg1=pd.read_csv(\"\")#Path to csv\n",
        "lgg1=pd.read_csv(\"\")\n",
        "\n",
        "lgg1[\"GRADE\"]=0\n",
        "hgg1[\"GRADE\"]=1\n",
        "hgg= hgg1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "lgg= lgg1.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "\n",
        "\n",
        "auc_lr=[]\n",
        "spe_lr=[]\n",
        "sen_lr=[]\n",
        "mcc_lr=[]\n",
        "acc_lr=[]\n",
        "f1_lr=[]\n",
        "pre_lr=[]\n",
        "rec_lr=[]\n",
        "pre_lrt=[]\n",
        "rec_lrt=[]\n",
        "auc_lrt=[]\n",
        "spe_lrt=[]\n",
        "mcc_lrt=[]\n",
        "sen_lrt=[]\n",
        "acc_lrt=[]\n",
        "f1_lrt=[]\n",
        "\n",
        "master=pd.concat([lgg, hgg])\n",
        "master= master.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "master= pd.concat([master[\"GRADE\"], master.drop(\"GRADE\", axis=1)], axis=1)\n",
        "Y=master[\"GRADE\"]\n",
        "master = (master - master.min()) / (master.max() - master.min())\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n",
        "for train_index, test_index in cv.split(master, Y):\n",
        "    #print(\"Train Index: \", train_index, \"\\n\")\n",
        "    #print(\"Test Index: \", test_index)\n",
        "\n",
        "    train, test = master.iloc[train_index], master.iloc[test_index]\n",
        "    Y_train, Y_test= Y.iloc[train_index], Y.iloc[test_index]\n",
        "    features=pymrmr.mRMR(train, 'MIQ', 43)\n",
        "    print(\"Best Features: \", features)\n",
        "\n",
        "    dataset=master[features]\n",
        "\n",
        "    X_train=dataset.iloc[train_index]\n",
        "    X_test=dataset.iloc[test_index]\n",
        "    \n",
        "    clf = XGBClassifier(n_estimators=450, max_depth=2, learning_rate=0.1)\n",
        "    clf.fit(X_train, Y_train)\n",
        "\n",
        "    pred = clf.predict(X_test)\n",
        "    cm = metrics.confusion_matrix(Y_test, pred)\n",
        "    TP = cm[1][1]\n",
        "    TN = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    sen_l = (TP / float(TP + FN))\n",
        "    spe_l = (TN / float(TN + FP))\n",
        "    pre_l = precision_score(Y_test, pred)\n",
        "    rec_l = recall_score(Y_test, pred)\n",
        "    mcc_l = matthews_corrcoef(Y_test, pred)\n",
        "    acc_l = accuracy_score(Y_test, pred)\n",
        "    probs = clf.predict_proba(X_test)\n",
        "    probs = probs[:, 1]\n",
        "    auc_l = roc_auc_score(Y_test, probs)\n",
        "    f1_l= f1_score(Y_test, pred)\n",
        "\n",
        "    \n",
        "    pred = clf.predict(X_train)\n",
        "    \n",
        "    cm = metrics.confusion_matrix(Y_train, pred)\n",
        "    TP = cm[1][1]\n",
        "    TN = cm[0][0]\n",
        "    FP = cm[0][1]\n",
        "    FN = cm[1][0]\n",
        "    sen_lt = (TP / float(TP + FN))\n",
        "    spe_lt = (TN / float(TN + FP))\n",
        "    pre_lt = precision_score(Y_train, pred)\n",
        "    rec_lt = recall_score(Y_train, pred)\n",
        "    mcc_lt = matthews_corrcoef(Y_train, pred)\n",
        "    acc_lt = accuracy_score(Y_train, pred)\n",
        "    probs = clf.predict_proba(X_train)\n",
        "    probs = probs[:, 1]\n",
        "    auc_lt = roc_auc_score(Y_train, probs)\n",
        "    f1_lt = f1_score(Y_train, pred)\n",
        "\n",
        "\n",
        "    auc_lr.append(auc_l)\n",
        "    spe_lr.append(spe_l)\n",
        "    sen_lr.append(sen_l)\n",
        "    mcc_lr.append(mcc_l)\n",
        "    acc_lr.append(acc_l)\n",
        "    f1_lr.append(f1_l)\n",
        "    rec_lr.append(rec_l)\n",
        "    pre_lr.append(pre_l)\n",
        "\n",
        "    pre_lrt.append(pre_lt)\n",
        "    rec_lrt.append(rec_lt)\n",
        "    auc_lrt.append(auc_lt)\n",
        "    spe_lrt.append(spe_lt)\n",
        "    sen_lrt.append(sen_lt)\n",
        "    mcc_lrt.append(mcc_lt)\n",
        "    acc_lrt.append(acc_lt)\n",
        "    f1_lrt.append(f1_lt)\n",
        "\n",
        "\n",
        "print(\"TRAIN:\")\n",
        "print(\"PRE:\", statistics.mean(pre_lrt))\n",
        "print(\"REC:\", statistics.mean(rec_lrt))\n",
        "print(\"SPE:\", statistics.mean(spe_lrt))\n",
        "print(\"MCC:\", statistics.mean(mcc_lrt))\n",
        "print(\"SEN:\", statistics.mean(sen_lrt))\n",
        "print(\"ACC:\", statistics.mean(acc_lrt))\n",
        "print(\"AUC:\", statistics.mean(auc_lrt))\n",
        "print(\"F1:\", statistics.mean(f1_lrt))\n",
        "\n",
        "print(\"TEST:\")\n",
        "print(\"PRE:\", statistics.mean(pre_lr))\n",
        "print(\"REC:\", statistics.mean(rec_lr))\n",
        "print(\"SPE:\", statistics.mean(spe_lr))\n",
        "print(\"MCC:\", statistics.mean(mcc_lr))\n",
        "print(\"SEN:\", statistics.mean(sen_lr))\n",
        "print(\"ACC:\", statistics.mean(acc_lr))\n",
        "print(\"AUC:\", statistics.mean(auc_lr))\n",
        "print(\"F1:\", statistics.mean(f1_lr))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}